{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "pd.options.display.max_columns = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test_bqCt9Pv.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"is_train\"] = 1\n",
    "test[\"is_train\"] = 0\n",
    "panel = pd.concat([train, test], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Date.of.Birth\"\n",
    "panel[col] = pd.to_datetime(panel[col], dayfirst=True, format=\"%d-%m-%y\")\n",
    "panel[col + \"_in_seconds\"] = (panel[col] - dt.datetime(1970,1,1)).dt.total_seconds()\n",
    "panel[col + \"_in_days\"] = (dt.datetime(2019, 1, 1) - panel[col]).dt.days\n",
    "panel[col + \"_year\"] = panel[col].dt.year\n",
    "panel[col + \"_month\"] = panel[col].dt.month\n",
    "\n",
    "col = \"DisbursalDate\"\n",
    "panel[col] = pd.to_datetime(panel[col], dayfirst=True, format=\"%d-%m-%y\")\n",
    "panel[\"DisbursalDay\"] = panel[\"DisbursalDate\"].dt.day\n",
    "panel[\"DisbursalDayofweek\"] = panel[\"DisbursalDate\"].dt.dayofweek\n",
    "panel[\"DisbursalMonth\"] = panel[\"DisbursalDate\"].dt.month # Not used in model\n",
    "\n",
    "panel.loc[panel[\"PERFORM_CNS.SCORE\"]< 300, \"PERFORM_CNS.SCORE\"] = 0\n",
    "\n",
    "tmp = pd.DataFrame(panel[\"AVERAGE.ACCT.AGE\"].str.findall(pat=\"\\d+\").tolist(), \n",
    "             columns=[\"years\", \"months\"]).astype(int)\n",
    "panel[\"AVERAGE.ACCT.AGE\"] = tmp[\"years\"] * 12 + tmp[\"months\"]\n",
    "\n",
    "tmp = pd.DataFrame(panel[\"CREDIT.HISTORY.LENGTH\"].str.findall(pat=\"\\d+\").tolist(), \n",
    "             columns=[\"years\", \"months\"]).astype(int)\n",
    "panel[\"CREDIT.HISTORY.LENGTH\"] = tmp[\"years\"] * 12 + tmp[\"months\"]\n",
    "\n",
    "column_combination = [\"PERFORM_CNS.SCORE.DESCRIPTION\"]\n",
    "column_combination_string = '_'.join(column_combination+['mean_score'])\n",
    "panel = pd.merge(panel, \n",
    "         panel.groupby(column_combination)['PERFORM_CNS.SCORE'].mean().to_frame(column_combination_string),\n",
    "         on=column_combination, how=\"left\")\n",
    "panel[column_combination_string+'_diff'] = panel[\"PERFORM_CNS.SCORE\"] - panel[column_combination_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employment.Type\n",
      "PERFORM_CNS.SCORE.DESCRIPTION\n",
      "employee_supplier\n"
     ]
    }
   ],
   "source": [
    "panel[\"employee_supplier\"] = panel[\"Employee_code_ID\"].astype(str) + \"_\" + panel[\"supplier_id\"].astype('str')\n",
    "for col in panel.columns:\n",
    "    if panel[col].dtype==object:\n",
    "        print(col)\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(panel[col].values.astype('str')) + list(panel[col].values.astype('str')))\n",
    "        panel[col] = lbl.transform(list(panel[col].values.astype('str')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col1, col2 in [\n",
    "                   [\"asset_cost\", \"disbursed_amount\"],\n",
    "                   [\"PRI.DISBURSED.AMOUNT\", \"PRI.CURRENT.BALANCE\"],\n",
    "                  ]:\n",
    "    panel[col1 + \"_diff_\" + col2] = panel[col1] - panel[col2]\n",
    "\n",
    "for col1, col2 in [[\"PRI.CURRENT.BALANCE\", \"PRI.DISBURSED.AMOUNT\"],\n",
    "                   [\"PRI.DISBURSED.AMOUNT\", \"PRI.SANCTIONED.AMOUNT\"],\n",
    "                   [\"AVERAGE.ACCT.AGE\", \"CREDIT.HISTORY.LENGTH\"],\n",
    "                   [\"PRI.ACTIVE.ACCTS\", \"PRI.NO.OF.ACCTS\"],\n",
    "                   [\"PRI.OVERDUE.ACCTS\", \"PRI.ACTIVE.ACCTS\"],\n",
    "                   [\"PRI.OVERDUE.ACCTS\", \"PRI.NO.OF.ACCTS\"],\n",
    "#                    [\"PRIMARY.INSTAL.AMT\", \"PRI.DISBURSED.AMOUNT\"],\n",
    "#                    [\"PRIMARY.INSTAL.AMT\", \"PRI.CURRENT.BALANCE\"],\n",
    "                   #[\"disbursed_amount\", \"PRI.CURRENT.BALANCE\"]\n",
    "                  ]:\n",
    "    panel[col1+\"_ratio_\"+col2] = panel[col1] / panel[col2]\n",
    "    \n",
    "    \n",
    "### Count features\n",
    "for col in [\"Current_pincode_ID\", \"Employee_code_ID\", \"supplier_id\", \"branch_id\", \"ltv\", \n",
    "            [\"supplier_id\", \"branch_id\"], [\"supplier_id\", \"Employee_code_ID\"],\n",
    "            [\"manufacturer_id\", \"branch_id\"], [\"manufacturer_id\", \"Employee_code_ID\"],\n",
    "            [\"Employment.Type\", \"Employee_code_ID\"], [\"Employment.Type\", \"branch_id\"],\n",
    "            [\"PERFORM_CNS.SCORE.DESCRIPTION\", \"Employee_code_ID\"],\n",
    "            [\"Current_pincode_ID\", \"Employee_code_ID\"],\n",
    "\n",
    "           ]:\n",
    "    if not isinstance(col, list):\n",
    "        col = [col]\n",
    "    col_name = \"_\".join(col)\n",
    "    all_df = panel[[\"UniqueID\"] + col].copy()\n",
    "    gdf = all_df.groupby(col)[\"UniqueID\"].count().reset_index()\n",
    "    gdf.columns = col + [col_name+\"_count\"]\n",
    "    panel = pd.merge(panel, gdf, on=col, how=\"left\")\n",
    "    \n",
    "    \n",
    "for col in [\"Current_pincode_ID\", \"Employee_code_ID\", \n",
    "            \"supplier_id\", ['branch_id', 'supplier_id', 'manufacturer_id'], \n",
    "            ['State_ID', 'Employee_code_ID']\n",
    "           ]:\n",
    "    if not isinstance(col, list):\n",
    "        col = [col]\n",
    "    col_name = \"_\".join(col)\n",
    "    all_df = panel[[\"ltv\"] + col].copy()\n",
    "    gdf = all_df.groupby(col)[\"ltv\"].agg([\"mean\", \"std\", \"max\"]).reset_index()\n",
    "    gdf.columns = col + [col_name+\"_ltv_mean\", col_name+\"_ltv_std\", col_name+\"_ltv_max\"]\n",
    "    panel = pd.merge(panel, gdf, on=col, how=\"left\")\n",
    "    panel[col_name+\"_ltv_mean\"+'_diff'] = panel[\"ltv\"] - panel[col_name+\"_ltv_mean\"]\n",
    "    \n",
    "\n",
    "for col in ['branch_id', 'supplier_id', 'manufacturer_id', 'Current_pincode_ID',\n",
    "              'State_ID', 'Employee_code_ID'\n",
    "           ]:\n",
    "    if not isinstance(col, list):\n",
    "        col = [col]\n",
    "    col_name = \"_\".join(col)\n",
    "    all_df = panel[[\"PERFORM_CNS.SCORE\"] + col].copy()\n",
    "    gdf = all_df.groupby(col)[\"PERFORM_CNS.SCORE\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    gdf.columns = col + [col_name+\"_performance_mean\", col_name+\"_performance_std\"]\n",
    "    panel = pd.merge(panel, gdf, on=col, how=\"left\")\n",
    "    \n",
    "    all_df = panel[[\"Date.of.Birth_in_seconds\"] + col].copy()\n",
    "    gdf = all_df.groupby(col)[\"Date.of.Birth_in_seconds\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "    gdf.columns = col + [col_name+\"_dob_mean\", col_name+\"_dob_std\"]\n",
    "    panel = pd.merge(panel, gdf, on=col, how=\"left\")\n",
    "    \n",
    "    all_df = panel[[\"disbursed_amount\"] + col].copy()\n",
    "    gdf = all_df.groupby(col)[\"disbursed_amount\"].agg([\"mean\", \"min\", \"max\", \"std\"]).reset_index()\n",
    "    gdf.columns = col + [col_name+\"_disamount_mean\", col_name+\"_disamount_min\", \n",
    "                         col_name+\"_disamount_max\", col_name+\"_disamount_std\"]\n",
    "    panel = pd.merge(panel, gdf, on=col, how=\"left\")\n",
    "    panel[col_name+\"_disamount_range\"] = (panel[col_name+\"_disamount_max\"] - panel[col_name+\"_disamount_min\"])\n",
    "    \n",
    "    all_df = panel[[\"DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS\"] + col].copy()\n",
    "    gdf = all_df.groupby(col)[\"DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS\"].mean().reset_index()\n",
    "    gdf.columns = col + [col_name+\"_delinq_mean\"]\n",
    "    panel = pd.merge(panel, gdf, on=col, how=\"left\")\n",
    "    \n",
    "    all_df = panel[[\"PRI.OVERDUE.ACCTS\"] + col].copy()\n",
    "    gdf = all_df.groupby(col)[\"PRI.OVERDUE.ACCTS\"].mean().reset_index()\n",
    "    gdf.columns = col + [col_name+\"_overdue_mean\"]\n",
    "    panel = pd.merge(panel, gdf, on=col, how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference between sum of all account ages and credit history length\n",
    "panel[\"diff_accountage_credithistory\"] = ((panel[\"PRI.ACTIVE.ACCTS\"] + panel[\"SEC.ACTIVE.ACCTS\"])\n",
    "                                          *panel[\"AVERAGE.ACCT.AGE\"]) - panel[\"CREDIT.HISTORY.LENGTH\"]\n",
    "\n",
    "# # Time elapsed\n",
    "# panel[\"time_elapsed\"] = (dt.datetime(2019, 1, 1) - panel[\"DisbursalDate\"]).dt.days\n",
    "\n",
    "# Ratio of disbursed amount to asset_cost\n",
    "panel[\"ratio_disbursedAmount_assetcost\"] = panel[\"disbursed_amount\"]/panel[\"asset_cost\"]\n",
    "\n",
    "# Documents availability\n",
    "panel[\"documents_available\"] = panel[[\"MobileNo_Avl_Flag\", \"Aadhar_flag\", \"PAN_flag\", \n",
    "                                      \"VoterID_flag\", \"Driving_flag\", \"Passport_flag\"]].sum(axis=1)\n",
    "\n",
    "# Property value\n",
    "panel[\"property_value\"] = (panel[\"disbursed_amount\"]*100)/panel[\"ltv\"]\n",
    "\n",
    "# PRIMARY+SECONDARY\n",
    "panel[\"TOTAL.NO.OF.ACCTS\"] = panel[\"PRI.NO.OF.ACCTS\"] + panel[\"SEC.NO.OF.ACCTS\"]\n",
    "panel[\"TOTAL.ACTIVE.ACCTS\"] = panel[\"PRI.ACTIVE.ACCTS\"] + panel[\"SEC.ACTIVE.ACCTS\"]\n",
    "panel[\"TOTAL.OVERDUE.ACCTS\"] = panel[\"PRI.OVERDUE.ACCTS\"] + panel[\"SEC.OVERDUE.ACCTS\"]\n",
    "panel[\"TOTAL.CURRENT.BALANCE\"] = panel[\"PRI.CURRENT.BALANCE\"] + panel[\"SEC.CURRENT.BALANCE\"]\n",
    "panel[\"TOTAL.SANCTIONED.AMOUNT\"] = panel[\"PRI.SANCTIONED.AMOUNT\"] + panel[\"SEC.SANCTIONED.AMOUNT\"]\n",
    "panel[\"TOTAL.DISBURSED.AMOUNT\"] = panel[\"PRI.DISBURSED.AMOUNT\"] + panel[\"SEC.DISBURSED.AMOUNT\"]\n",
    "panel[\"TOTAL.INSTAL.AMT\"] = panel[\"PRIMARY.INSTAL.AMT\"] + panel[\"SEC.INSTAL.AMT\"]\n",
    "\n",
    "# Diff sanctioned amount and disbursed amount\n",
    "panel[\"diff_sanctionedamount_disbursedamount\"] = (panel[\"PRI.SANCTIONED.AMOUNT\"] + \n",
    "                                                 panel[\"SEC.SANCTIONED.AMOUNT\"] -\n",
    "                                                 panel[\"PRI.DISBURSED.AMOUNT\"] - \n",
    "                                                 panel[\"SEC.DISBURSED.AMOUNT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233154, 165) (112392, 165)\n"
     ]
    }
   ],
   "source": [
    "columns_for_model = np.setdiff1d(panel.columns.values, [\"UniqueID\", \n",
    "                                                          \"Date.of.Birth\", \"DisbursalDate\", \"DisbursalMonth\",\n",
    "                                                          \"loan_default\",\n",
    "                                                          \"is_train\"]).tolist()\n",
    "train_X = panel.loc[panel[\"is_train\"] == 1, columns_for_model+[\"loan_default\", \"DisbursalMonth\"]].reset_index(drop=True)\n",
    "test_X = panel.loc[panel[\"is_train\"] == 0, [\"UniqueID\"]+columns_for_model].reset_index(drop=True)\n",
    "train_y = train_X[\"loan_default\"].values\n",
    "train_groups = train_X[\"DisbursalMonth\"].values\n",
    "train_X = train_X.drop([\"loan_default\", \"DisbursalMonth\"], axis=1)\n",
    "test_ids = test_X[\"UniqueID\"].values\n",
    "test_X = test_X.drop([\"UniqueID\"], axis=1)\n",
    "print(train_X.shape, test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None, dep=8, seed=0, data_leaf=511, hessian_leaf=50):\n",
    "    params = {}\n",
    "#     params[\"objective\"] = \"binary\"\n",
    "#     params['metric'] = 'auc'\n",
    "#     params[\"max_depth\"] = dep\n",
    "#     params[\"num_leaves\"] = 30\n",
    "#     params[\"min_data_in_leaf\"] = data_leaf\n",
    "#     params[\"learning_rate\"] = 0.01\n",
    "#     params[\"bagging_fraction\"] = 0.8\n",
    "#     params[\"feature_fraction\"] = 0.35\n",
    "#     params[\"feature_fraction_seed\"] = seed\n",
    "#     params[\"bagging_freq\"] = 1\n",
    "#     params[\"bagging_seed\"] = seed\n",
    "#     params[\"lambda_l2\"] = 5\n",
    "#     params[\"lambda_l1\"] = 5\n",
    "#     params[\"verbosity\"] = -1\n",
    "    \n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary\"\n",
    "    params['metric'] = 'auc'\n",
    "#     params[\"max_depth\"] = dep\n",
    "    params[\"num_leaves\"] = 31\n",
    "    params[\"min_data_in_leaf\"] = data_leaf\n",
    "    params[\"min_sum_hessian_in_leaf\"] = hessian_leaf\n",
    "    params[\"learning_rate\"] = 0.01\n",
    "    params[\"bagging_fraction\"] = 0.8\n",
    "    params[\"feature_fraction\"] = 0.2\n",
    "    params[\"feature_fraction_seed\"] = seed\n",
    "    params[\"bagging_freq\"] = 5\n",
    "    params[\"bagging_seed\"] = seed\n",
    "    params[\"lambda_l2\"] = 0.95\n",
    "    params[\"lambda_l1\"] = 0.95\n",
    "    params[\"verbosity\"] = -1\n",
    "    num_rounds = 20000\n",
    "\n",
    "    plst = list(params.items())\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=200, verbose_eval=500)\n",
    "    else:\n",
    "        lgtest = lgb.DMatrix(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    #imps = model.feature_importance()\n",
    "    #names = model.feature_name()\n",
    "    #for fi, fn in enumerate(names):\n",
    "    #    print(fn, imps[fi])\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = roc_auc_score(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return model, loss, pred_test_y, pred_test_y2\n",
    "    else:\n",
    "        return model, loss, pred_test_y, pred_test_y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model..\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.660728\n",
      "[1000]\tvalid_0's auc: 0.665714\n",
      "[1500]\tvalid_0's auc: 0.66713\n",
      "[2000]\tvalid_0's auc: 0.667454\n",
      "Early stopping, best iteration is:\n",
      "[1930]\tvalid_0's auc: 0.667555\n",
      "0.6675611624804539\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.660625\n",
      "[1000]\tvalid_0's auc: 0.665604\n",
      "[1500]\tvalid_0's auc: 0.667068\n",
      "[2000]\tvalid_0's auc: 0.667552\n",
      "Early stopping, best iteration is:\n",
      "[2196]\tvalid_0's auc: 0.667704\n",
      "0.6677066851035663\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.660992\n",
      "[1000]\tvalid_0's auc: 0.665912\n",
      "[1500]\tvalid_0's auc: 0.667413\n",
      "[2000]\tvalid_0's auc: 0.667926\n",
      "Early stopping, best iteration is:\n",
      "[2114]\tvalid_0's auc: 0.667983\n",
      "0.6679866400057071\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.67316\n",
      "[1000]\tvalid_0's auc: 0.677652\n",
      "[1500]\tvalid_0's auc: 0.6788\n",
      "[2000]\tvalid_0's auc: 0.679362\n",
      "[2500]\tvalid_0's auc: 0.679492\n",
      "Early stopping, best iteration is:\n",
      "[2715]\tvalid_0's auc: 0.67973\n",
      "0.6797195110584032\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.67371\n",
      "[1000]\tvalid_0's auc: 0.677954\n",
      "[1500]\tvalid_0's auc: 0.679348\n",
      "[2000]\tvalid_0's auc: 0.679692\n",
      "[2500]\tvalid_0's auc: 0.679995\n",
      "Early stopping, best iteration is:\n",
      "[2795]\tvalid_0's auc: 0.680106\n",
      "0.6800967435775218\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.673002\n",
      "[1000]\tvalid_0's auc: 0.677558\n",
      "[1500]\tvalid_0's auc: 0.679001\n",
      "[2000]\tvalid_0's auc: 0.679626\n",
      "[2500]\tvalid_0's auc: 0.679931\n",
      "Early stopping, best iteration is:\n",
      "[2751]\tvalid_0's auc: 0.679976\n",
      "0.6799687741305012\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.674394\n",
      "[1000]\tvalid_0's auc: 0.680102\n",
      "[1500]\tvalid_0's auc: 0.681834\n",
      "[2000]\tvalid_0's auc: 0.682516\n",
      "[2500]\tvalid_0's auc: 0.682775\n",
      "Early stopping, best iteration is:\n",
      "[2632]\tvalid_0's auc: 0.682845\n",
      "0.6828439045753711\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.675006\n",
      "[1000]\tvalid_0's auc: 0.680226\n",
      "[1500]\tvalid_0's auc: 0.681951\n",
      "[2000]\tvalid_0's auc: 0.682686\n",
      "[2500]\tvalid_0's auc: 0.683097\n",
      "Early stopping, best iteration is:\n",
      "[2701]\tvalid_0's auc: 0.683115\n",
      "0.683113373568043\n",
      "Training until validation scores don't improve for 200 rounds.\n",
      "[500]\tvalid_0's auc: 0.674446\n",
      "[1000]\tvalid_0's auc: 0.679933\n",
      "[1500]\tvalid_0's auc: 0.681582\n",
      "[2000]\tvalid_0's auc: 0.682382\n",
      "[2500]\tvalid_0's auc: 0.682754\n",
      "Early stopping, best iteration is:\n",
      "[2682]\tvalid_0's auc: 0.682839\n",
      "0.6828363243051273\n",
      "0.6771936908644441\n"
     ]
    }
   ],
   "source": [
    "print(\"Building model..\")\n",
    "cv_scores = []\n",
    "pred_test_full = 0\n",
    "pred_train = np.zeros(train_X.shape[0])\n",
    "n_splits = 3\n",
    "#kf = model_selection.KFold(n_splits=n_splits, shuffle=True, random_state=7988)\n",
    "gkf = GroupKFold(n_splits=n_splits)\n",
    "model_name = \"lgb\"\n",
    "for dev_index, val_index in gkf.split(train_X, train_y, train_groups):\n",
    "    dev_X, val_X = train_X.iloc[dev_index,:], train_X.iloc[val_index,:]\n",
    "    dev_y, val_y = train_y[dev_index], train_y[val_index]\n",
    "\n",
    "    pred_val = 0\n",
    "    pred_test = 0\n",
    "    n_models = 0.\n",
    "\n",
    "    model, loss, pred_v, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, seed=2019)\n",
    "    pred_val += pred_v\n",
    "    pred_test += pred_t\n",
    "    n_models += 1\n",
    "    \n",
    "    model, loss, pred_v, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, data_leaf=450, hessian_leaf=30, seed=9873)\n",
    "    pred_val += pred_v\n",
    "    pred_test += pred_t\n",
    "    n_models += 1\n",
    "    \n",
    "    model, loss, pred_v, pred_t = runLGB(dev_X, dev_y, val_X, val_y, test_X, data_leaf=600, hessian_leaf=70, seed=4568)\n",
    "    pred_val += pred_v\n",
    "    pred_test += pred_t\n",
    "    n_models += 1\n",
    "    \n",
    "    pred_val /= n_models\n",
    "    pred_test /= n_models\n",
    "    \n",
    "    loss = roc_auc_score(val_y, pred_val)\n",
    "        \n",
    "    pred_train[val_index] = pred_val\n",
    "    pred_test_full += pred_test / n_splits\n",
    "    cv_scores.append(loss)\n",
    "#     break\n",
    "print(np.mean(cv_scores))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = pd.DataFrame(test_ids, columns=[\"UniqueID\"])\n",
    "sub_df[\"loan_default\"] = pred_test_full\n",
    "sub_df.to_csv(\"sub_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
